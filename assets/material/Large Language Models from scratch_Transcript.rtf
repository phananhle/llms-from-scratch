{\rtf1\ansi\ansicpg1252\cocoartf2706
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww17800\viewh21380\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Large Language Models from scratch_Transcript\
\
Hello everyone. In this video, you'll learn all about large language models. Let's start with that autocomplete feature on your mobile phone. Did you ever wonder how it works? The suggested word here is 'the'. It's the most used word in the English language.\
\
Let's type 'y' next. There are a number of words that start with 't y'. If you took the shortest, you get 't y e'. This graph plots the frequency of this word over time. It was pretty popular in 1806 but today, you'd have to read 20 million words to find your first occurrence of 'tye'. We can look up these frequencies for every word starting with 't y'. If we sort by word frequency, we have a clear winner.\
\
The same approach works for phrases and sentences. You see it every time you start typing into your internet search engine. The search engine scores each query by calculating its frequency, in other words, how many other people have used that query.\
\
So, is that all there is to language modeling? Indeed, the goal is to assign a probability to every sentence, and frequencies are one way to calculate probabilities. The problem is that this frequency approach doesn't allow you to score new sentences. This is a perfectly valid sentence but one that may not have appeared previously. You may ask, how many new sentences are there really? Given the zillions of internet posts each day, won't we exhaust all possible combinations of words soon?\
\
Let's do a back of the envelope calculation. There are well over a hundred thousand words in the English language and a typical sentence has more than 10 words. That means 10 to the 50th combinations. This is a ridiculously big number. The vast majority of sentences will never be seen by any human. So, to really model language, we need to do more than just count sentences that already exist. We need to somehow model things like grammar and style.\
\
Example: Here's an example from a Nobel Prize-winning poet. Let's try to build a language model that can write like Bob Dylan. We'll start by thinking of this text as a time series where each word depends on the previous one. Notice that the word 'was' appears three times. Let's merge all three instances into one. This turns our series into a graph. The other word that repeats is 'if'. Let's merge these two as well. If we add probability to these edges, it becomes a language model and you can use it to generate text in the style of Bob Dylan. If you start it early and traverse the right sequence of branches, you'll get the original verses.\
\
Now, let's generate a brand new phrase that wasn't in the song. We'll start at 'the' and follow these branches. There's another example - hey, these are actually pretty good! They are brand-new phrases that sound like Bob Dylan. But other paths give bizarre results, and most just produce nonsense. How can we make this better? For starters, we can use a lot more text to build our model. Here's what you get if you build the model using the whole song. Hmm, these are still pretty weird. The real problem here is that our model is too simplistic. It assumes that each word depends only on the previous word.\
\
We can write this as a conditional probability - it's the probability of the current word 'x of n' conditioned on the previous word 'x of n minus one'. You can do a little better if you look at relationships of three words instead of just two. Let's build a table of all consecutive triples, these are called trigrams.\
\
And you can use these trigrams to define next word probabilities based on the two previous words. The text generation results are slightly better, but still not great. The problem is that these words can have pretty long-range dependencies. For example, the word 'red' relates to 'hair', which is three words back, but it also rhymes with 'bed', which is 13 words back. If you ignored those rhymes, the song wouldn't work at all.\
\
Neural Networks: So we need to model functions like this or even longer to accurately represent language, and these functions are exceedingly complex. There's no way we can model this exactly, but we can try to approximate it. There are a number of ways to approximate a function. You may have heard of Fourier series, which approximates a function using sines and cosines, or Taylor series which is a sum of polynomials. These are both universal approximators - they can approximate almost any function.\
\
Another example of a universal approximator is neural networks. An advantage of neural networks is that you don't need to know anything about the function you're trying to approximate - just input and output pairs.\
\
Let's try an example. This function is kind of interesting. Let's approximate it with a neural network. We'll use this one. It has five nodes and eight weights.\
\
Let's choose an 'x' position on the graph and send that through the network. The first layer duplicates 'x' and multiplies each copy with a different weight. Then, each weighted copy is passed through an activation function - an s-shaped curve called a 'sigmoid' in this case. Multiply by a new weight, and then add it up.\
\
The result is our approximation of 'f of x', which we'll call 'y'. We can plot it to see how far it is from the function we're trying to fit. This is our error. You can see it's pretty big. That's because I generated the weights randomly.\
\
So this is just one data point. We can send many 'x' values to our network to generate this red curve. Let's define an error function that adds up all of these differences between red and blue curve values. We'll use these errors to update the weights.\
\
This is called 'training the network', and if we repeat these update steps thousands of times, we get a pretty good fit. We can think of our energy function as a landscape. Our goal is to find the lowest point - the basin. That seems easy, right? But what if I didn't show you the function? It's a lot harder now, huh?\
\
Okay, I'll give you a few points, and also their negative gradients, which point downhill. Now, you can just follow the gradients to roll down the hill. This is called 'gradient descent'. It's how neural networks are optimized.\
\
So to train our network, we need the gradient of the error function. It's a vector with eight partial derivatives, one for each weight. This turns out to be pretty straightforward to compute for neural networks and you can do it in a single backwards path through the network.\
\
This process of calculating partial derivatives in a network is called 'back propagation', and it's the workhorse of neural networks. Alright, now you just told me that neural networks are universal approximators - you can fit any function.\
\
So, how about this function which models language? Well, there's some network that can do it, but it needs enough capacity. As an analogy, suppose we tried fitting our blue curve with this network which only has four weights. It can't fit that second bump because there's not enough capacity in the network.\
\
And the design decisions matter. Like what if I used a different activation function? 'Relus' are pretty popular, but they give piecewise linear reconstructions. So you need more of them to fit a curvy function like this.\
\
So, how do we go about designing a neural network to model language? Stay tuned for part two, where we create an amazing neural network that can generate poetry, translate languages, and even write computer code.}